{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m1kemp/ecgDiagnosis/blob/main/%CE%91%CE%BD%CF%84%CE%AF%CE%B3%CF%81%CE%B1%CF%86%CE%BF_DownLoadPhysionetDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Il-hgAcf6yi",
        "outputId": "95c0bb6b-3c5f-4dcb-9f8b-bf27c90ce748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "Download complete! Unzipping...\n",
            "Dataset is ready.\n",
            "Deleted zip file: data/training2017.zip\n",
            "Downloading dataset...\n",
            "Download complete! Unzipping...\n",
            "Dataset is ready.\n",
            "Deleted zip file: data/mit-bih-arrhythmia-database-1.0.0.zip\n"
          ]
        }
      ],
      "source": [
        "#DownLoad PhysioNet and MIT-BIH datasets\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# URL of PhysioNet dataset\n",
        "DATA_URL_PHYSIONET = \"https://physionet.org/static/published-projects/challenge-2017/1.0.0/training2017.zip\"\n",
        "SAVE_PATH_PHYSIONET = \"data/training2017.zip\"\n",
        "\n",
        "# URL of MIT-BIH dataset\n",
        "DATA_URL_MITBIH = \"https://physionet.org/static/published-projects/mitdb/mit-bih-arrhythmia-database-1.0.0.zip\"\n",
        "SAVE_PATH_MITBIH = \"data/mit-bih-arrhythmia-database-1.0.0.zip\"\n",
        "\n",
        "def download_data(DATA_URL, SAVE_PATH):\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(DATA_URL, stream=True)\n",
        "\n",
        "    with open(SAVE_PATH, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Download complete! Unzipping...\")\n",
        "    os.system(f\"unzip {SAVE_PATH} -d data/\")\n",
        "    print(\"Dataset is ready.\")\n",
        "\n",
        "    # Delete the zip file\n",
        "    os.remove(SAVE_PATH)\n",
        "    print(f\"Deleted zip file: {SAVE_PATH}\")\n",
        "\n",
        "download_data(DATA_URL_PHYSIONET, SAVE_PATH_PHYSIONET)\n",
        "download_data(DATA_URL_MITBIH, SAVE_PATH_MITBIH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8cA-5Uo5bZq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_OUbktMiJUY",
        "outputId": "5dc45a38-61e6-4de9-91bd-2b6b06aea763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting wfdb\n",
            "  Using cached wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.15)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.15.2)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.20.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.4.26)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Using cached wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: pandas, wfdb\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas wfdb-4.3.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting neurokit2\n",
            "  Downloading neurokit2-0.2.10-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.0.2)\n",
            "Collecting pandas (from neurokit2)\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from neurokit2) (3.10.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->neurokit2) (1.17.0)\n",
            "Downloading neurokit2-0.2.10-py2.py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.1/693.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: pandas, neurokit2\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed neurokit2-0.2.10 pandas\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting biosppy\n",
            "  Downloading biosppy-2.2.3-py2.py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting bidict (from biosppy)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biosppy) (3.13.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from biosppy) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biosppy) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.15.2)\n",
            "Collecting shortuuid (from biosppy)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.4.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from biosppy) (4.11.0.86)\n",
            "Collecting pywavelets (from biosppy)\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting mock (from biosppy)\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (2.9.0.post0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->biosppy) (3.6.0)\n",
            "Downloading biosppy-2.2.3-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: shortuuid, pywavelets, mock, bidict, biosppy\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed bidict-0.23.1 biosppy-2.2.3 mock-5.2.0 pywavelets-1.8.0 shortuuid-1.0.13\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gzip\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting peakutils\n",
            "  Downloading PeakUtils-1.3.5-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from peakutils) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from peakutils) (1.15.2)\n",
            "Downloading PeakUtils-1.3.5-py3-none-any.whl (7.7 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: peakutils\n",
            "Successfully installed peakutils-1.3.5\n",
            "PhysioNet Data Shape: torch.Size([340517, 1250])\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wfdb\n",
        "!pip install neurokit2\n",
        "!pip install biosppy\n",
        "!pip install torch\n",
        "!pip install pickle\n",
        "!pip install gzip\n",
        "!pip install peakutils\n",
        "\n",
        "# Preprocessing for Physionet Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wfdb\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import neurokit2 as nk\n",
        "import biosppy.signals.ecg as ecg\n",
        "import torch\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "# Define constants\n",
        "TARGET_SAMPLING_RATE = 125  # Hz\n",
        "MAX_LEN_PHYSIONET = 10 * TARGET_SAMPLING_RATE  # 10 seconds\n",
        "MAX_LEN_MITBIH = 30 * TARGET_SAMPLING_RATE  # 30 seconds\n",
        "\n",
        "# Load PhysioNet dataset\n",
        "def load_physionet_data(path):\n",
        "    \"\"\"Load PhysioNet 2017 dataset from .mat files and reference.csv.\"\"\"\n",
        "    signals, labels = [], []\n",
        "    ref_df = pd.read_csv(os.path.join(path, \"REFERENCE.csv\"), header=None)\n",
        "    ref_dict = dict(zip(ref_df[0], ref_df[1]))\n",
        "    label_mapping = {\"N\": 0, \"A\": 1, \"O\": 2, \"~\": 3}  # Modify as per dataset classes\n",
        "    \n",
        "\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith(\".mat\"):\n",
        "            record_name = file.replace(\".mat\", \"\")\n",
        "            mat_data = scipy.io.loadmat(os.path.join(path, file))\n",
        "            signal = mat_data[\"val\"][0]  # Extract ECG lead\n",
        "            label = ref_dict.get(record_name, None)\n",
        "            if label:\n",
        "                signals.append(signal)\n",
        "                physionet_labels = label_mapping[label]\n",
        "                labels.append(physionet_labels)\n",
        "\n",
        "    return signals, labels\n",
        "\n",
        "\n",
        "\n",
        "# Downsampling function\n",
        "def downsample_signal(signal, original_fs, target_fs=125):\n",
        "    \"\"\"Downsample ECG signal from original_fs to target_fs.\"\"\"\n",
        "    num_samples = int(len(signal) * target_fs / original_fs)\n",
        "    return scipy.signal.resample(signal, num_samples)\n",
        "\n",
        "# Normalize function\n",
        "def normalize_signal(signal):\n",
        "    \"\"\"Normalize ECG signal between 0 and 1.\"\"\"\n",
        "    return (signal - np.min(signal)) / (np.max(signal) - np.min(signal))\n",
        "\n",
        "# R-peak detection\n",
        "def detect_r_peaks(signal, sampling_rate=125):\n",
        "    \"\"\"Detect R-peaks using the Pan-Tompkins algorithm.\"\"\"\n",
        "    _, r_peaks = nk.ecg_peaks(signal, sampling_rate=sampling_rate)\n",
        "    return np.array(r_peaks[\"ECG_R_Peaks\"])\n",
        "# Extract T-episodes\n",
        "def extract_t_episodes(signal, r_peaks, fs):\n",
        "    \"\"\"T-episodes are intervals centered on R-peaks of length median(R-R interval).\"\"\"\n",
        "    if len(r_peaks) < 2:\n",
        "        return []\n",
        "    rr_intervals = np.diff(r_peaks)\n",
        "    median_rr = int(np.median(rr_intervals))\n",
        "    episodes = []\n",
        "    for r in r_peaks:\n",
        "        start = max(0, r - median_rr // 2)\n",
        "        end = min(len(signal), r + median_rr // 2)\n",
        "        episodes.append((start, end))\n",
        "    return episodes\n",
        "\n",
        "# Zero-padding\n",
        "def pad_signal(signal, max_len=MAX_LEN_PHYSIONET):\n",
        "    \"\"\"Pad signal to max_len with zeros.\"\"\"\n",
        "    if len(signal) < max_len:\n",
        "        return np.pad(signal, (0, max_len - len(signal)), 'constant')\n",
        "    else:\n",
        "        return signal[:max_len]\n",
        "\n",
        "# Full preprocessing pipeline\n",
        "def preprocess_ecg_dataset(dataset_path):\n",
        "    \"\"\"Preprocess ECG dataset from PhysioNet or MIT-BIH.\"\"\"\n",
        "    signals, labels = load_physionet_data(dataset_path)\n",
        "    original_fs = 300  # PhysioNet signals are sampled at 300 Hz\n",
        "    max_len = MAX_LEN_PHYSIONET\n",
        "    processed_signals,processed_labels = [],[]\n",
        "    for signal,label in zip(signals,labels):\n",
        "        # 1. Downsampling\n",
        "        signal = downsample_signal(signal, original_fs, TARGET_SAMPLING_RATE)\n",
        "        # 2. Normalization\n",
        "        signal = normalize_signal(signal)\n",
        "        # 3. R-peak detection\n",
        "        r_peaks = detect_r_peaks(signal, TARGET_SAMPLING_RATE)\n",
        "        #4. T-episode and beat extraction\n",
        "        t_eps = extract_t_episodes(signal, r_peaks, TARGET_SAMPLING_RATE)\n",
        "        for start, end in t_eps:\n",
        "            beat = signal[start:end]\n",
        "            processed_signals.append(pad_signal(beat, max_len))\n",
        "            processed_labels.append(label)\n",
        "\n",
        "    return np.array(processed_signals), np.array(processed_labels)\n",
        "\n",
        "# Example Usage\n",
        "physionet_data, physionet_labels = preprocess_ecg_dataset(\"data/training2017\")\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_physionet = torch.tensor(physionet_data, dtype=torch.float32)\n",
        "y_physionet = torch.tensor(physionet_labels, dtype=torch.long)\n",
        "\n",
        "#save the processed data\n",
        "# Save data with gzip compression\n",
        "with gzip.open(\"pretraining_data.pkl.gz\", \"wb\") as f:\n",
        "    pickle.dump((X_physionet, y_physionet), f)\n",
        "print(f\"PhysioNet Data Shape: {X_physionet.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbM9jefcjxRn",
        "outputId": "2eedb356-3649-4d57-d528-041c1f427b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing NNModel.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile NNModel.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# My Model\n",
        "# Squeeze-and-Excitation (SE) Module\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, _ = x.shape\n",
        "        se = self.global_avg_pool(x).view(batch, channels)  # Global Avg Pool\n",
        "        se = F.relu(self.fc1(se))  # First FC + ReLU\n",
        "        se = F.softmax(self.fc2(se), dim=1)  # Second FC + Softmax\n",
        "        se = se.view(batch, channels, 1)\n",
        "        return x * se  # Scale the input\n",
        "\n",
        "# Residual Block with SE Module\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2)  # Add MaxPooling layer\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
        "        self.se = SEBlock(out_channels)\n",
        "        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.maxpool(x)  # Apply MaxPooling\n",
        "        residual = self.shortcut(x)  # Skip connection\n",
        "        out = self.bn1(F.relu(self.conv1(x)))\n",
        "        out = self.conv2(out)\n",
        "        out = self.se(out)  # Apply SE Module\n",
        "        out += residual  # Add Skip Connection\n",
        "        return F.relu(out)\n",
        "\n",
        "# Full Model: CNN + BiLSTM + FC\n",
        "class ECGClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(ECGClassifier, self).__init__()\n",
        "\n",
        "        # Initial Convolutional Layers\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # Residual Blocks with SE Module (4 Blocks)\n",
        "        self.resblock1 = ResidualBlock(32, 64)\n",
        "        self.resblock2 = ResidualBlock(64, 96)\n",
        "        self.resblock3 = ResidualBlock(96, 128)\n",
        "        self.resblock4 = ResidualBlock(128, 160)\n",
        "\n",
        "        # MaxPooling Layer\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # BiLSTM Layers\n",
        "        self.lstm = nn.LSTM(160, 64, num_layers=2, bidirectional=True, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(128, 64)  # BiLSTM outputs 64*2=128\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "         # First Conv -> ReLU -> BatchNorm\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # Second Conv -> ReLU -> BatchNorm\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        # Residual Blocks with SE Module\n",
        "        x = self.resblock1(x)\n",
        "        x = self.resblock2(x)\n",
        "        x = self.resblock3(x)\n",
        "        x = self.resblock4(x)\n",
        "\n",
        "\n",
        "        # Reshape for LSTM (Batch, SeqLen, Features)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # BiLSTM\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout1(x[:, -1, :])  # Take last LSTM output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)  # Softmax for classification\n",
        "\n",
        "# Create Model Instance\n",
        "model = ECGClassifier(num_classes=5)\n",
        "\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9LZbhFj3DJ",
        "outputId": "6f5b44ca-83e4-4a6d-e4ed-72175b087d53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nbimporter in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~andas (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mECGClassifier(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (resblock1): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
            "      (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (resblock2): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(64, 96, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=96, out_features=6, bias=True)\n",
            "      (fc2): Linear(in_features=6, out_features=96, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(64, 96, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (resblock3): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(96, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
            "      (fc2): Linear(in_features=8, out_features=128, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(96, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (resblock4): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(128, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=160, out_features=10, bias=True)\n",
            "      (fc2): Linear(in_features=10, out_features=160, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(128, 160, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (lstm): LSTM(160, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (dropout1): Dropout(p=0.2, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
            ")\n",
            "Using device: cuda\n",
            "Epoch 1/3, Loss: 0.91910826442968\n",
            "Epoch 2/3, Loss: 0.8105014346972643\n",
            "Epoch 3/3, Loss: 0.7806745423530864\n",
            "Pretraining Complete. Model Saved as pretrained_model.pth.gz!\n"
          ]
        }
      ],
      "source": [
        "# Pretraining with PhysioNet Data\n",
        "!pip install nbimporter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nbimporter\n",
        "from NNModel import ECGClassifier\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# Load Pretraining Data (PhysioNet)\n",
        "with gzip.open(\"pretraining_data.pkl.gz\", \"rb\") as f:\n",
        "    X_train, y_train = pickle.load(f)\n",
        "\n",
        "# Convert to PyTorch Dataset\n",
        "class ECGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.unsqueeze(1)  # Add channel dimension\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 64\n",
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Modify Model for Pretraining (Last FC Layer = 4 Classes)\n",
        "class PretrainECGClassifier(ECGClassifier):\n",
        "    def __init__(self):\n",
        "        super(PretrainECGClassifier, self).__init__(num_classes=4)  # Change output classes to 4\n",
        "\n",
        "# Initialize Model\n",
        "model = PretrainECGClassifier()\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "# He Normal Initialization\n",
        "def he_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "\n",
        "model.apply(he_init)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Pretraining Loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Save Pretrained Model with gzip and pickle\n",
        "with gzip.open(\"pretrained_model.pth.gz\", \"wb\") as f:\n",
        "    pickle.dump(model.state_dict(), f)\n",
        "print(\"Pretraining Complete. Model Saved as pretrained_model.pth.gz!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvv_IKAAwII_",
        "outputId": "c65f89ed-bf95-4c27-f3f6-ba71912d12e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIT-BIH Beats Shape: torch.Size([109966, 250])\n",
            "Unique Labels: (array([0, 1, 2, 3, 4]), array([90631,  2781,  7708,   803,  8043]))\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing for MIT-BIH Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import wfdb\n",
        "import gzip\n",
        "import pickle\n",
        "import neurokit2 as nk\n",
        "from scipy.signal import resample\n",
        "\n",
        "# Constants\n",
        "TARGET_SAMPLING_RATE = 125\n",
        "MAX_LEN_MITBIH = 30 * TARGET_SAMPLING_RATE  # 30 seconds\n",
        "\n",
        "# Beat label mapping\n",
        "label_mapping = {\n",
        "    'N': 0, 'L': 0, 'R': 0,\n",
        "    'V': 3, '/': 4,\n",
        "    'A': 2, 'F': 3, 'f': 4,\n",
        "    'j': 2, 'a': 2, 'E': 3,\n",
        "    'J': 2, 'e': 2, 'Q': 4,\n",
        "    'S': 2\n",
        "}\n",
        "\n",
        "def load_mitbih_record(record_path):\n",
        "    record = wfdb.rdrecord(record_path)\n",
        "    annotation = wfdb.rdann(record_path, 'atr')\n",
        "    signal = record.p_signal[:, 0]  # use first ECG lead\n",
        "    return signal, annotation.sample, annotation.symbol\n",
        "\n",
        "def downsample_signal(signal, original_fs=360, target_fs=125):\n",
        "    num_samples = int(len(signal) * target_fs / original_fs)\n",
        "    return resample(signal, num_samples)\n",
        "\n",
        "def normalize_signal(signal):\n",
        "    return (signal - np.min(signal)) / (np.max(signal) - np.min(signal) + 1e-6)\n",
        "\n",
        "def detect_r_peaks(signal, fs=125):\n",
        "    \"\"\"Use NeuroKit2 to detect R-peaks.\"\"\"\n",
        "    _, rpeaks = nk.ecg_peaks(signal, sampling_rate=fs)\n",
        "    return rpeaks[\"ECG_R_Peaks\"]\n",
        "\n",
        "def extract_t_episodes(signal, r_peaks, fs):\n",
        "    if len(r_peaks) < 2:\n",
        "        return []\n",
        "    rr_intervals = np.diff(r_peaks)\n",
        "    median_rr = int(np.median(rr_intervals))\n",
        "    episodes = []\n",
        "    for r in r_peaks:\n",
        "        start = max(0, r - median_rr // 2)\n",
        "        end = min(len(signal), r + median_rr // 2)\n",
        "        episodes.append((start, end))\n",
        "    return episodes\n",
        "\n",
        "def assign_labels_to_episodes(episodes, ann_samples, ann_symbols, label_map):\n",
        "    labels = []\n",
        "    for start, end in episodes:\n",
        "        center = (start + end) // 2\n",
        "        nearest_idx = np.argmin(np.abs(np.array(ann_samples) - center))\n",
        "        label = ann_symbols[nearest_idx]\n",
        "        if label in label_map:\n",
        "            labels.append(label_map[label])\n",
        "        else:\n",
        "            labels.append(None)\n",
        "    return labels\n",
        "\n",
        "def pad_signal(signal, max_len):\n",
        "    if len(signal) < max_len:\n",
        "        return np.pad(signal, (0, max_len - len(signal)), 'constant')\n",
        "    else:\n",
        "        return signal[:max_len]\n",
        "\n",
        "def preprocess_mitbih_dataset(dataset_dir):\n",
        "    all_beats, all_labels = [], []\n",
        "    for file in os.listdir(dataset_dir):\n",
        "        if file.endswith('.dat'):\n",
        "            record_name = file.replace('.dat', '')\n",
        "            signal, ann_samples, ann_symbols = load_mitbih_record(os.path.join(dataset_dir, record_name))\n",
        "            # Downsample signal\n",
        "            signal = downsample_signal(signal)\n",
        "            ann_samples = (np.array(ann_samples) * (TARGET_SAMPLING_RATE / 360)).astype(int)\n",
        "            # Normalize\n",
        "            signal = normalize_signal(signal)\n",
        "            # R-peak detection using NeuroKit\n",
        "            r_peaks = detect_r_peaks(signal, TARGET_SAMPLING_RATE)\n",
        "            # Extract T-episodes\n",
        "            t_episodes = extract_t_episodes(signal, r_peaks, TARGET_SAMPLING_RATE)\n",
        "            # Match to labels\n",
        "            labels = assign_labels_to_episodes(t_episodes, ann_samples, ann_symbols, label_mapping)\n",
        "            for (start, end), label in zip(t_episodes, labels):\n",
        "                if label is not None:\n",
        "                    beat = signal[start:end]\n",
        "                    padded = pad_signal(beat, MAX_LEN_MITBIH)\n",
        "                    all_beats.append(padded)\n",
        "                    all_labels.append(label)\n",
        "    return np.array(all_beats), np.array(all_labels)\n",
        "\n",
        "# Preprocess MIT-BIH\n",
        "mitbih_signals, mitbih_labels = preprocess_mitbih_dataset(\"data/mit-bih-arrhythmia-database-1.0.0\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_mitbih = torch.tensor(mitbih_signals, dtype=torch.float32)\n",
        "y_mitbih = torch.tensor(mitbih_labels, dtype=torch.long)\n",
        "\n",
        "# Save\n",
        "with gzip.open(\"mitbih_beats.pkl.gz\", \"wb\") as f:\n",
        "    pickle.dump((X_mitbih, y_mitbih), f)\n",
        "\n",
        "print(f\"MIT-BIH Beats Shape: {X_mitbih.shape}\")\n",
        "print(f\"Unique Labels: {np.unique(mitbih_labels, return_counts=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbR2IjQu_e2y",
        "outputId": "436145f3-5fb1-4712-97b4-421bf1f5843f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/ColabData/ECG/mit-bih-arrhythmia-database.zip /content/\n",
        "!cp /content/drive/MyDrive/ColabData/ECG/pretrained_model.pth.gz /content/\n",
        "!cp /content/drive/MyDrive/ColabData/ECG/pretraining_data.pkl.gz /content/ # Corrected file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLzo5XwRqWjo",
        "outputId": "4b7199cb-3264-4c08-f710-8deea7b43461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/100] Loss: 0.3340\n",
            "[Epoch 2/100] Loss: 0.1164\n",
            "[Epoch 3/100] Loss: 0.0867\n",
            "[Epoch 4/100] Loss: 0.0680\n",
            "[Epoch 5/100] Loss: 0.0614\n",
            "[Epoch 6/100] Loss: 0.0525\n",
            "[Epoch 7/100] Loss: 0.0460\n",
            "[Epoch 8/100] Loss: 0.0432\n",
            "[Epoch 9/100] Loss: 0.0384\n",
            "[Epoch 10/100] Loss: 0.0346\n",
            "[Epoch 11/100] Loss: 0.0330\n",
            "[Epoch 12/100] Loss: 0.0283\n",
            "[Epoch 13/100] Loss: 0.0263\n",
            "[Epoch 14/100] Loss: 0.0252\n",
            "[Epoch 15/100] Loss: 0.0259\n",
            "[Epoch 16/100] Loss: 0.0220\n",
            "[Epoch 17/100] Loss: 0.0243\n",
            "[Epoch 18/100] Loss: 0.0217\n",
            "[Epoch 19/100] Loss: 0.0213\n",
            "[Epoch 20/100] Loss: 0.0191\n",
            "[Epoch 21/100] Loss: 0.0181\n",
            "[Epoch 22/100] Loss: 0.0158\n",
            "[Epoch 23/100] Loss: 0.0179\n",
            "[Epoch 24/100] Loss: 0.0184\n",
            "[Epoch 25/100] Loss: 0.0151\n",
            "[Epoch 26/100] Loss: 0.0138\n",
            "[Epoch 27/100] Loss: 0.0140\n",
            "[Epoch 28/100] Loss: 0.0155\n",
            "[Epoch 29/100] Loss: 0.0126\n",
            "[Epoch 30/100] Loss: 0.0135\n",
            "[Epoch 31/100] Loss: 0.0162\n",
            "[Epoch 32/100] Loss: 0.0146\n",
            "[Epoch 33/100] Loss: 0.0115\n",
            "[Epoch 34/100] Loss: 0.0117\n",
            "[Epoch 35/100] Loss: 0.0105\n",
            "[Epoch 36/100] Loss: 0.0118\n",
            "[Epoch 37/100] Loss: 0.0101\n",
            "[Epoch 38/100] Loss: 0.0115\n",
            "[Epoch 39/100] Loss: 0.0131\n",
            "[Epoch 40/100] Loss: 0.0095\n",
            "[Epoch 41/100] Loss: 0.0075\n",
            "[Epoch 42/100] Loss: 0.0126\n",
            "[Epoch 43/100] Loss: 0.0099\n",
            "[Epoch 44/100] Loss: 0.0098\n",
            "[Epoch 45/100] Loss: 0.0095\n",
            "[Epoch 46/100] Loss: 0.0081\n",
            "[Epoch 47/100] Loss: 0.0118\n",
            "[Epoch 48/100] Loss: 0.0097\n",
            "[Epoch 49/100] Loss: 0.0090\n",
            "[Epoch 50/100] Loss: 0.0068\n",
            "[Epoch 51/100] Loss: 0.0082\n",
            "[Epoch 52/100] Loss: 0.0081\n",
            "[Epoch 53/100] Loss: 0.0077\n",
            "[Epoch 54/100] Loss: 0.0082\n",
            "[Epoch 55/100] Loss: 0.0086\n",
            "[Epoch 56/100] Loss: 0.0072\n",
            "[Epoch 57/100] Loss: 0.0077\n",
            "[Epoch 58/100] Loss: 0.0066\n",
            "[Epoch 59/100] Loss: 0.0067\n",
            "[Epoch 60/100] Loss: 0.0079\n",
            "[Epoch 61/100] Loss: 0.0072\n",
            "[Epoch 62/100] Loss: 0.0070\n",
            "[Epoch 63/100] Loss: 0.0078\n",
            "[Epoch 64/100] Loss: 0.0069\n",
            "[Epoch 65/100] Loss: 0.0044\n",
            "[Epoch 66/100] Loss: 0.0091\n",
            "[Epoch 67/100] Loss: 0.0058\n",
            "[Epoch 68/100] Loss: 0.0068\n",
            "[Epoch 69/100] Loss: 0.0060\n",
            "[Epoch 70/100] Loss: 0.0093\n",
            "[Epoch 71/100] Loss: 0.0068\n",
            "[Epoch 72/100] Loss: 0.0050\n",
            "[Epoch 73/100] Loss: 0.0071\n",
            "[Epoch 74/100] Loss: 0.0064\n",
            "[Epoch 75/100] Loss: 0.0048\n",
            "[Epoch 76/100] Loss: 0.0062\n",
            "[Epoch 77/100] Loss: 0.0049\n",
            "[Epoch 78/100] Loss: 0.0073\n",
            "[Epoch 79/100] Loss: 0.0060\n",
            "[Epoch 80/100] Loss: 0.0045\n",
            "[Epoch 81/100] Loss: 0.0048\n",
            "[Epoch 82/100] Loss: 0.0051\n",
            "[Epoch 83/100] Loss: 0.0046\n",
            "[Epoch 84/100] Loss: 0.0051\n",
            "[Epoch 85/100] Loss: 0.0039\n",
            "[Epoch 86/100] Loss: 0.0041\n",
            "[Epoch 87/100] Loss: 0.0065\n",
            "[Epoch 88/100] Loss: 0.0044\n",
            "[Epoch 89/100] Loss: 0.0090\n",
            "[Epoch 90/100] Loss: 0.0047\n",
            "[Epoch 91/100] Loss: 0.0032\n",
            "[Epoch 92/100] Loss: 0.0048\n",
            "[Epoch 93/100] Loss: 0.0059\n",
            "[Epoch 94/100] Loss: 0.0044\n",
            "[Epoch 95/100] Loss: 0.0067\n",
            "[Epoch 96/100] Loss: 0.0070\n",
            "[Epoch 97/100] Loss: 0.0035\n",
            "[Epoch 98/100] Loss: 0.0033\n",
            "[Epoch 99/100] Loss: 0.0035\n",
            "[Epoch 100/100] Loss: 0.0037\n",
            "\n",
            "=== Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9954    0.9957    0.9956     18127\n",
            "           1     0.9487    0.8975    0.9224       556\n",
            "           2     0.9621    0.9870    0.9744      1542\n",
            "           3     0.8421    0.8000    0.8205       160\n",
            "           4     0.9988    0.9944    0.9966      1609\n",
            "\n",
            "    accuracy                         0.9911     21994\n",
            "   macro avg     0.9494    0.9349    0.9419     21994\n",
            "weighted avg     0.9910    0.9911    0.9910     21994\n",
            "\n",
            "=== Confusion Matrix ===\n",
            "[[18049    27    32    17     2]\n",
            " [   49   499     8     0     0]\n",
            " [   13     0  1522     7     0]\n",
            " [   16     0    16   128     0]\n",
            " [    5     0     4     0  1600]]\n",
            "Multiclass ROC-AUC Score: 0.9949\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score,accuracy_score,precision_score, recall_score, f1_score,roc_curve, auc\n",
        "from NNModel import ECGClassifier  # Make sure this is implemented correctly\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------\n",
        "# 1. Load and Split Data\n",
        "# --------------------\n",
        "with gzip.open(\"mitbih_beats.pkl.gz\", \"rb\") as f:\n",
        "    X_mit, y_mit = pickle.load(f)\n",
        "\n",
        "# Train-Test Split (Stratified, 80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_mit, y_mit, test_size=0.2, random_state=42, stratify=y_mit\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# 2. Define Dataset Class\n",
        "# --------------------\n",
        "class ECGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.unsqueeze(1)  # Shape: (B, 1, L)\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# --------------------\n",
        "# 3. Create DataLoaders\n",
        "# --------------------\n",
        "batch_size = 250\n",
        "train_loader = torch.utils.data.DataLoader(ECGDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(ECGDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --------------------\n",
        "# 4. Model Initialization\n",
        "# --------------------\n",
        "model = ECGClassifier(num_classes=5)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load Pretrained Weights (excluding last FC layer)\n",
        "with gzip.open(\"pretrained_model.pth.gz\", \"rb\") as f:\n",
        "    pretrained_dict = pickle.load(f)\n",
        "\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith('fc2')}\n",
        "model.load_state_dict(pretrained_dict, strict=False)\n",
        "\n",
        "# Replace last FC layer\n",
        "model.fc2 = nn.Linear(model.fc2.in_features, 5).to(device)\n",
        "\n",
        "# --------------------\n",
        "# 5. Training Setup\n",
        "# --------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "num_epochs = 100\n",
        "\n",
        "# --------------------\n",
        "# 6. Training Loop\n",
        "# --------------------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# --------------------\n",
        "# 7. Testing / Evaluation\n",
        "# --------------------\n",
        "# model.eval()\n",
        "# all_preds = []\n",
        "# all_probs = []\n",
        "# all_labels = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for X_batch, y_batch in test_loader:\n",
        "#         X_batch = X_batch.to(device)\n",
        "#         outputs = model(X_batch)\n",
        "#         probs = F.softmax(outputs, dim=1)\n",
        "#         preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "#         all_probs.extend(probs.cpu().numpy())\n",
        "#         all_preds.extend(preds.cpu().numpy())\n",
        "#         all_labels.extend(y_batch.numpy())\n",
        "\n",
        "# # Convert lists to arrays\n",
        "# all_labels = np.array(all_labels)\n",
        "# all_preds = np.array(all_preds)\n",
        "# all_probs = np.array(all_probs)\n",
        "\n",
        "# # Print Metrics\n",
        "# print(\"\\n=== Classification Report ===\")\n",
        "# print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# print(\"=== Confusion Matrix ===\")\n",
        "# print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "\n",
        "# #Print Accuracy\n",
        "# print(\"\\n=== Accuracy ===\")\n",
        "# print( accuracy_score(all_labels, all_preds))\n",
        "\n",
        "# # Print Precision\n",
        "# print(\"\\n=== Precision ===\")\n",
        "# print(precision_score(all_labels, all_preds, average='weighted'))\n",
        "\n",
        "# # Print Recall\n",
        "# print(\"\\n=== Recall ===\")\n",
        "# print(recall_score(all_labels, all_preds, average='weighted'))\n",
        "# # Print F1 Score\n",
        "# print(\"\\n=== F1 Score ===\")\n",
        "# print(f1_score(all_labels, all_preds, average='weighted'))\n",
        "\n",
        "# try:\n",
        "#     y_true_bin = np.eye(5)[all_labels]\n",
        "#     auc_score = roc_auc_score(y_true_bin, all_probs, multi_class='ovr')\n",
        "#     print(f\"Multiclass ROC-AUC Score: {auc_score:.4f}\")\n",
        "# except ValueError as e:\n",
        "#     print(\"ROC-AUC calculation skipped:\", e)\n",
        "\n",
        "# for i in range(5):  # for 5 classes\n",
        "#     fpr, tpr, _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
        "#     class_auc = auc(fpr, tpr)\n",
        "#     print(f\"Class {i} AUC: {class_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curves\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# for i in range(5):\n",
        "#     fpr, tpr, _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
        "#     roc_auc = auc(fpr, tpr)\n",
        "#     plt.plot(fpr, tpr, label=f\"ROC class {i} (area = {roc_auc:.2f})\")\n",
        "\n",
        "# plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
        "# plt.xlabel(\"False Positive Rate\")\n",
        "# plt.ylabel(\"True Positive Rate\")\n",
        "# plt.title(\"ROC Curves of All Classes\")\n",
        "# plt.legend(loc=\"lower right\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds, all_probs, all_labels = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y_batch.numpy())\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\n=== Accuracy ===\", accuracy_score(all_labels, all_preds))\n",
        "print(\"\\n=== Precision ===\", precision_score(all_labels, all_preds, average='weighted'))\n",
        "print(\"\\n=== Recall ===\", recall_score(all_labels, all_preds, average='weighted'))\n",
        "print(\"\\n=== F1 Score ===\", f1_score(all_labels, all_preds, average='weighted'))\n",
        "\n",
        "try:\n",
        "    y_true_bin = np.eye(5)[all_labels]\n",
        "    auc_score = roc_auc_score(y_true_bin, all_probs, multi_class='ovr')\n",
        "    print(f\"Multiclass ROC-AUC Score: {auc_score:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(\"ROC-AUC calculation skipped:\", e)\n",
        "\n",
        "for i in range(5):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
        "    class_auc = auc(fpr, tpr)\n",
        "    print(f\"Class {i} AUC: {class_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Class labels (adjust if your class labels are different)\n",
        "class_names = ['N', 'S', 'V', 'F', 'Q']\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
        "            xticklabels=class_names, yticklabels=class_names, cbar=True)\n",
        "\n",
        "plt.title(\"(a) Confusion matrix of Kachuee's model\", fontsize=14)\n",
        "plt.xlabel(\"Predicted label\", fontsize=12)\n",
        "plt.ylabel(\"True label\", fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ROC Curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(5):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], all_probs[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"ROC class {i} (area = {roc_auc:.2f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves of All Classes\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m1kemp/ecgDiagnosis/blob/main/%CE%91%CE%BD%CF%84%CE%AF%CE%B3%CF%81%CE%B1%CF%86%CE%BF_DownLoadPhysionetDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Il-hgAcf6yi",
        "outputId": "52da4c3c-43ed-4275-b1a2-301a7918eb42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n",
            "Download complete! Unzipping...\n",
            "Dataset is ready.\n",
            "Deleted zip file: data/training2017.zip\n",
            "Downloading dataset...\n",
            "Download complete! Unzipping...\n",
            "Dataset is ready.\n",
            "Deleted zip file: data/mit-bih-arrhythmia-database-1.0.0.zip\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# URL of PhysioNet dataset\n",
        "DATA_URL_PHYSIONET = \"https://physionet.org/static/published-projects/challenge-2017/1.0.0/training2017.zip\"\n",
        "SAVE_PATH_PHYSIONET = \"data/training2017.zip\"\n",
        "\n",
        "# URL of MIT-BIH dataset\n",
        "DATA_URL_MITBIH = \"https://physionet.org/static/published-projects/mitdb/mit-bih-arrhythmia-database-1.0.0.zip\"\n",
        "SAVE_PATH_MITBIH = \"data/mit-bih-arrhythmia-database-1.0.0.zip\"\n",
        "\n",
        "def download_data(DATA_URL, SAVE_PATH):\n",
        "    if not os.path.exists(\"data\"):\n",
        "        os.makedirs(\"data\")\n",
        "\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(DATA_URL, stream=True)\n",
        "\n",
        "    with open(SAVE_PATH, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(\"Download complete! Unzipping...\")\n",
        "    os.system(f\"unzip {SAVE_PATH} -d data/\")\n",
        "    print(\"Dataset is ready.\")\n",
        "\n",
        "    # Delete the zip file\n",
        "    os.remove(SAVE_PATH)\n",
        "    print(f\"Deleted zip file: {SAVE_PATH}\")\n",
        "\n",
        "download_data(DATA_URL_PHYSIONET, SAVE_PATH_PHYSIONET)\n",
        "download_data(DATA_URL_MITBIH, SAVE_PATH_MITBIH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8cA-5Uo5bZq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_OUbktMiJUY",
        "outputId": "f4b08d74-39fe-4364-8f45-2f50458b3e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.14.1)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.2.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.3/162.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 wfdb-4.2.0\n",
            "Collecting neurokit2\n",
            "  Downloading neurokit2-0.2.10-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from neurokit2) (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from neurokit2) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from neurokit2) (3.10.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->neurokit2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->neurokit2) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->neurokit2) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->neurokit2) (1.17.0)\n",
            "Downloading neurokit2-0.2.10-py2.py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.1/693.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.2.10\n",
            "Collecting biosppy\n",
            "  Downloading biosppy-2.2.3-py2.py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting bidict (from biosppy)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from biosppy) (3.13.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from biosppy) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biosppy) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.14.1)\n",
            "Collecting shortuuid (from biosppy)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from biosppy) (1.4.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from biosppy) (4.11.0.86)\n",
            "Collecting pywavelets (from biosppy)\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting mock (from biosppy)\n",
            "  Downloading mock-5.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->biosppy) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->biosppy) (3.6.0)\n",
            "Downloading biosppy-2.2.3-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading mock-5.2.0-py3-none-any.whl (31 kB)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: shortuuid, pywavelets, mock, bidict, biosppy\n",
            "Successfully installed bidict-0.23.1 biosppy-2.2.3 mock-5.2.0 pywavelets-1.8.0 shortuuid-1.0.13\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement gzip (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gzip\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting peakutils\n",
            "  Downloading PeakUtils-1.3.5-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from peakutils) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from peakutils) (1.14.1)\n",
            "Downloading PeakUtils-1.3.5-py3-none-any.whl (7.7 kB)\n",
            "Installing collected packages: peakutils\n",
            "Successfully installed peakutils-1.3.5\n",
            "PhysioNet Data Shape: torch.Size([340517, 1250])\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n",
        "!pip install neurokit2\n",
        "!pip install biosppy\n",
        "!pip install torch\n",
        "!pip install pickle\n",
        "!pip install gzip\n",
        "!pip install peakutils\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wfdb\n",
        "import scipy.io\n",
        "import scipy.signal\n",
        "import neurokit2 as nk\n",
        "import biosppy.signals.ecg as ecg\n",
        "import torch\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# Define constants\n",
        "TARGET_SAMPLING_RATE = 125  # Hz\n",
        "MAX_LEN_PHYSIONET = 10 * TARGET_SAMPLING_RATE  # 10 seconds\n",
        "MAX_LEN_MITBIH = 30 * TARGET_SAMPLING_RATE  # 30 seconds\n",
        "\n",
        "# Load PhysioNet dataset\n",
        "def load_physionet_data(path):\n",
        "    \"\"\"Load PhysioNet 2017 dataset from .mat files and reference.csv.\"\"\"\n",
        "    signals, labels = [], []\n",
        "    ref_df = pd.read_csv(os.path.join(path, \"REFERENCE.csv\"), header=None)\n",
        "    ref_dict = dict(zip(ref_df[0], ref_df[1]))\n",
        "    label_mapping = {\"N\": 0, \"A\": 1, \"O\": 2, \"~\": 3}  # Modify as per dataset classes\n",
        "\n",
        "\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith(\".mat\"):\n",
        "            record_name = file.replace(\".mat\", \"\")\n",
        "            mat_data = scipy.io.loadmat(os.path.join(path, file))\n",
        "            signal = mat_data[\"val\"][0]  # Extract ECG lead\n",
        "            label = ref_dict.get(record_name, None)\n",
        "            if label:\n",
        "                signals.append(signal)\n",
        "                physionet_labels = label_mapping[label]\n",
        "                labels.append(physionet_labels)\n",
        "\n",
        "    return signals, labels\n",
        "\n",
        "\n",
        "\n",
        "# Downsampling function\n",
        "def downsample_signal(signal, original_fs, target_fs=125):\n",
        "    \"\"\"Downsample ECG signal from original_fs to target_fs.\"\"\"\n",
        "    num_samples = int(len(signal) * target_fs / original_fs)\n",
        "    return scipy.signal.resample(signal, num_samples)\n",
        "\n",
        "# Normalize function\n",
        "def normalize_signal(signal):\n",
        "    \"\"\"Normalize ECG signal between 0 and 1.\"\"\"\n",
        "    return (signal - np.min(signal)) / (np.max(signal) - np.min(signal))\n",
        "\n",
        "# R-peak detection\n",
        "def detect_r_peaks(signal, sampling_rate=125):\n",
        "    \"\"\"Detect R-peaks using the Pan-Tompkins algorithm.\"\"\"\n",
        "    _, r_peaks = nk.ecg_peaks(signal, sampling_rate=sampling_rate)\n",
        "    return np.array(r_peaks[\"ECG_R_Peaks\"])\n",
        "\n",
        "# Beat segmentation\n",
        "def extract_beats(signal, r_peaks, window_size=0.5, fs=125):\n",
        "    \"\"\"Extract ECG beats centered around R-peaks.\"\"\"\n",
        "    beat_length = int(window_size * fs)\n",
        "    beats = []\n",
        "    for peak in r_peaks:\n",
        "        start = max(0, peak - beat_length)\n",
        "        end = min(len(signal), peak + beat_length)\n",
        "        beat = signal[start:end]\n",
        "        beats.append(beat)\n",
        "    return beats\n",
        "\n",
        "# Zero-padding\n",
        "def pad_signal(signal, max_len=MAX_LEN_PHYSIONET):\n",
        "    \"\"\"Pad signal to max_len with zeros.\"\"\"\n",
        "    if len(signal) < max_len:\n",
        "        return np.pad(signal, (0, max_len - len(signal)), 'constant')\n",
        "    else:\n",
        "        return signal[:max_len]\n",
        "\n",
        "# Full preprocessing pipeline\n",
        "def preprocess_ecg_dataset(dataset_path):\n",
        "    \"\"\"Preprocess ECG dataset from PhysioNet or MIT-BIH.\"\"\"\n",
        "    signals, labels = load_physionet_data(dataset_path)\n",
        "    original_fs = 300  # PhysioNet signals are sampled at 300 Hz\n",
        "    max_len = MAX_LEN_PHYSIONET\n",
        "    processed_signals,processed_labels = [],[]\n",
        "    for signal,label in zip(signals,labels):\n",
        "        # 1. Downsampling\n",
        "        signal = downsample_signal(signal, original_fs, TARGET_SAMPLING_RATE)\n",
        "        # 2. Normalization\n",
        "        signal = normalize_signal(signal)\n",
        "        # 3. R-peak detection\n",
        "        r_peaks = detect_r_peaks(signal, TARGET_SAMPLING_RATE)\n",
        "        # 4. Beat extraction\n",
        "        beats = extract_beats(signal, r_peaks)\n",
        "        # 5. Zero-padding each beat\n",
        "        padded_beats = [pad_signal(beat, max_len) for beat in beats]\n",
        "        processed_signals.extend(padded_beats)  # Collect all beats\n",
        "        processed_labels.extend([label] * len(padded_beats))  # Assign the same label to all extracted beats\n",
        "\n",
        "    return np.array(processed_signals), np.array(processed_labels)\n",
        "\n",
        "# Example Usage\n",
        "physionet_data, physionet_labels = preprocess_ecg_dataset(\"data/training2017\")\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_physionet = torch.tensor(physionet_data, dtype=torch.float32)\n",
        "y_physionet = torch.tensor(physionet_labels, dtype=torch.long)\n",
        "\n",
        "#save the processed data\n",
        "# Save data with gzip compression\n",
        "with gzip.open(\"pretraining_data.pkl.gz\", \"wb\") as f:\n",
        "    pickle.dump((X_physionet, y_physionet), f)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"PhysioNet Data Shape: {X_physionet.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbM9jefcjxRn",
        "outputId": "7ebf74b4-87c2-4852-bf4d-4aa394c44a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing NNModel.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile NNModel.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Squeeze-and-Excitation (SE) Module\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, _ = x.shape\n",
        "        se = self.global_avg_pool(x).view(batch, channels)  # Global Avg Pool\n",
        "        se = F.relu(self.fc1(se))  # First FC + ReLU\n",
        "        se = F.softmax(self.fc2(se), dim=1)  # Second FC + Softmax\n",
        "        se = se.view(batch, channels, 1)\n",
        "        return x * se  # Scale the input\n",
        "\n",
        "# Residual Block with SE Module\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2)  # Add MaxPooling layer\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2)\n",
        "        self.se = SEBlock(out_channels)\n",
        "        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.maxpool(x)  # Apply MaxPooling\n",
        "        residual = self.shortcut(x)  # Skip connection\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.conv2(out)\n",
        "        out = self.se(out)  # Apply SE Module\n",
        "        out += residual  # Add Skip Connection\n",
        "        return F.relu(out)\n",
        "\n",
        "# Full Model: CNN + BiLSTM + FC\n",
        "class ECGClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(ECGClassifier, self).__init__()\n",
        "\n",
        "        # Initial Convolutional Layers\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # Residual Blocks with SE Module (4 Blocks)\n",
        "        self.resblock1 = ResidualBlock(32, 64)\n",
        "        self.resblock2 = ResidualBlock(64, 96)\n",
        "        self.resblock3 = ResidualBlock(96, 128)\n",
        "        self.resblock4 = ResidualBlock(128, 160)\n",
        "\n",
        "        # MaxPooling Layer\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # BiLSTM Layers\n",
        "        self.lstm = nn.LSTM(160, 64, num_layers=2, bidirectional=True, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(128, 64)  # BiLSTM outputs 64*2=128\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "         # First Conv -> ReLU -> BatchNorm\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # Second Conv -> ReLU -> BatchNorm\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        # Residual Blocks with SE Module\n",
        "        x = self.resblock1(x)\n",
        "        x = self.resblock2(x)\n",
        "        x = self.resblock3(x)\n",
        "        x = self.resblock4(x)\n",
        "\n",
        "        # Max Pooling\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Reshape for LSTM (Batch, SeqLen, Features)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # BiLSTM\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout1(x[:, -1, :])  # Take last LSTM output\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)  # Softmax for classification\n",
        "\n",
        "# Create Model Instance\n",
        "model = ECGClassifier(num_classes=5)\n",
        "\n",
        "print(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9LZbhFj3DJ",
        "outputId": "98458215-43f1-41b1-80a3-9889674b2737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nbimporter in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "ECGClassifier(\n",
            "  (conv1): Conv1d(1, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (resblock1): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=64, out_features=4, bias=True)\n",
            "      (fc2): Linear(in_features=4, out_features=64, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (resblock2): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(64, 96, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(96, 96, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=96, out_features=6, bias=True)\n",
            "      (fc2): Linear(in_features=6, out_features=96, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(64, 96, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (resblock3): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(96, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=128, out_features=8, bias=True)\n",
            "      (fc2): Linear(in_features=8, out_features=128, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(96, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (resblock4): ResidualBlock(\n",
            "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv1d(128, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (bn1): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Conv1d(160, 160, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (se): SEBlock(\n",
            "      (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n",
            "      (fc1): Linear(in_features=160, out_features=10, bias=True)\n",
            "      (fc2): Linear(in_features=10, out_features=160, bias=True)\n",
            "    )\n",
            "    (shortcut): Conv1d(128, 160, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (lstm): LSTM(160, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (dropout1): Dropout(p=0.2, inplace=False)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
            ")\n",
            "Using device: cuda\n",
            "Epoch 1/3, Loss: 1.0226958345994983\n",
            "Epoch 2/3, Loss: 1.0168445234884782\n",
            "Epoch 3/3, Loss: 1.016095334057341\n",
            "Pretraining Complete. Model Saved as pretrained_model.pth.gz!\n"
          ]
        }
      ],
      "source": [
        "!pip install nbimporter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nbimporter\n",
        "from NNModel import ECGClassifier\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# Load Pretraining Data (PhysioNet)\n",
        "with gzip.open(\"pretraining_data.pkl.gz\", \"rb\") as f:\n",
        "    X_train, y_train = pickle.load(f)\n",
        "\n",
        "# Convert to PyTorch Dataset\n",
        "class ECGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.unsqueeze(1)  # Add channel dimension\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 64\n",
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Modify Model for Pretraining (Last FC Layer = 4 Classes)\n",
        "class PretrainECGClassifier(ECGClassifier):\n",
        "    def __init__(self):\n",
        "        super(PretrainECGClassifier, self).__init__(num_classes=4)  # Change output classes to 4\n",
        "\n",
        "# Initialize Model\n",
        "model = PretrainECGClassifier()\n",
        "\n",
        "# Use CUDA if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "\n",
        "# He Normal Initialization\n",
        "def he_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
        "\n",
        "model.apply(he_init)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Pretraining Loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Save Pretrained Model with gzip and pickle\n",
        "with gzip.open(\"pretrained_model.pth.gz\", \"wb\") as f:\n",
        "    pickle.dump(model.state_dict(), f)\n",
        "print(\"Pretraining Complete. Model Saved as pretrained_model.pth.gz!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbR2IjQu_e2y",
        "outputId": "436145f3-5fb1-4712-97b4-421bf1f5843f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp /content/drive/MyDrive/ColabData/ECG/mit-bih-arrhythmia-database.zip /content/\n",
        "!cp /content/drive/MyDrive/ColabData/ECG/pretrained_model.pth.gz /content/\n",
        "!cp /content/drive/MyDrive/ColabData/ECG/pretraining_data.pkl.gz /content/ # Corrected file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import gzip\n",
        "from sklearn.model_selection import train_test_split\n",
        "from NNModel import ECGClassifier\n",
        "\n",
        "# Load MIT-BIH Data\n",
        "with gzip.open(\"mitbih_beats.pkl.gz\", \"rb\") as f:\n",
        "    X_mit, y_mit = pickle.load(f)\n",
        "\n",
        "# Train/Test Split (80/20)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mit, y_mit, test_size=0.2, random_state=42, stratify=y_mit)\n",
        "\n",
        "# Dataset\n",
        "class ECGDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.unsqueeze(1)  # (B, 1, L)\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 250\n",
        "train_loader = torch.utils.data.DataLoader(ECGDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(ECGDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model Init\n",
        "model = ECGClassifier(num_classes=5)  # MIT-BIH has 5 beat classes\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load pretrained weights (optional)\n",
        "with gzip.open(\"pretrained_model.pth.gz\", \"rb\") as f:\n",
        "    state_dict = pickle.load(f)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, 5).to(device)  # Update FC layer\n",
        "\n",
        "# Optimizer: Adam (with specified betas)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "\n",
        "# Loss Function: CrossEntropy\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
